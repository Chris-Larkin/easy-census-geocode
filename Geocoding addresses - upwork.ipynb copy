{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T17:58:23.270418Z",
     "start_time": "2021-06-20T17:58:22.576984Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import random\n",
    "import math\n",
    "import censusgeocode\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from IPython.display import display, clear_output\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T17:58:23.276486Z",
     "start_time": "2021-06-20T17:58:23.272940Z"
    }
   },
   "outputs": [],
   "source": [
    "chunk_size = 650\n",
    "errors_chunck_size = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T17:58:24.710667Z",
     "start_time": "2021-06-20T17:58:23.279390Z"
    }
   },
   "outputs": [],
   "source": [
    "k_columns = ['num_street','city','state','zip_code']\n",
    "cachefile = Path('.') / 'cache-dict.csv'\n",
    "cache = {}\n",
    "cache_df = None\n",
    "if cachefile.exists():\n",
    "    cache = pickle.loads(cachefile.read_bytes())\n",
    "    ds = []\n",
    "    for k,v in cache.items():\n",
    "        d = {}\n",
    "        d.update({k:v for k,v in zip(k_columns, k)})\n",
    "        d.update(v)\n",
    "        ds.append(d)\n",
    "    cache_df = pd.DataFrame(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T17:58:24.717352Z",
     "start_time": "2021-06-20T17:58:24.713232Z"
    }
   },
   "outputs": [],
   "source": [
    "def safe_print(*args, sep=\" \", end=\"\", **kwargs):\n",
    "    '''\n",
    "    more thread safe print()\n",
    "    '''\n",
    "    \n",
    "    joined_string = sep.join([str(arg) for arg in args])\n",
    "    print(joined_string  + \"\\n\", sep=sep, end=end, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T17:58:24.930692Z",
     "start_time": "2021-06-20T17:58:24.719047Z"
    }
   },
   "outputs": [],
   "source": [
    "cache_lock = threading.Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T17:58:25.366444Z",
     "start_time": "2021-06-20T17:58:24.934936Z"
    }
   },
   "outputs": [],
   "source": [
    "df_original = pd.read_csv('dta/For_geocoding_mv.csv', dtype={'id': 'str'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T17:58:25.443621Z",
     "start_time": "2021-06-20T17:58:25.369668Z"
    }
   },
   "outputs": [],
   "source": [
    "# make sure there is no duplicated id\n",
    "assert df_original['id'].duplicated().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T17:58:27.724427Z",
     "start_time": "2021-06-20T17:58:25.445979Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df_original.copy()\n",
    "df = df.set_index(k_columns)\n",
    "for c in ['address','match','matchtype','parsed','tigerlineid','side','statefp','countyfp','tract','block','lat','lon']:\n",
    "    df[c] = None\n",
    "if cache is not None:\n",
    "    df.update(cache_df.set_index(k_columns))\n",
    "df = df.reset_index().set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T17:58:27.873532Z",
     "start_time": "2021-06-20T17:58:27.727135Z"
    }
   },
   "outputs": [],
   "source": [
    "df_with_cache = df\n",
    "df = df_with_cache[df['match'].isnull()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T17:58:27.883293Z",
     "start_time": "2021-06-20T17:58:27.876500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 2690\n",
      "Chunk size: 650\n",
      "Number of chunks: 5\n"
     ]
    }
   ],
   "source": [
    "n_chunks = math.ceil(df.shape[0] / chunk_size)\n",
    "\n",
    "print('Total rows:', df.shape[0])\n",
    "print('Chunk size:', chunk_size)\n",
    "print('Number of chunks:', n_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T17:58:27.958354Z",
     "start_time": "2021-06-20T17:58:27.885350Z"
    }
   },
   "outputs": [],
   "source": [
    "def addressbatch_retry(i, data, sleep=30, att=3):\n",
    "    '''\n",
    "    call cg.addressbatch() and retry 'att' times if it fails\n",
    "    '''\n",
    "    cg = censusgeocode.CensusGeocode()\n",
    "    time.sleep(random.randint(1,40) / 10)\n",
    "    data = data.read()\n",
    "    while att > 0:\n",
    "        try:\n",
    "            att -= 1\n",
    "            r = cg.addressbatch(io.StringIO(data), timeout=8*60)\n",
    "            return r\n",
    "        except Exception as ex:\n",
    "            safe_print('ERROR:', ex)\n",
    "            if att > 0:\n",
    "                safe_print(f'Waiting {sleep} secs before trying again. Remaing {att} attempts.')\n",
    "                time.sleep(sleep)\n",
    "                sleep *= 2\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T17:58:28.034197Z",
     "start_time": "2021-06-20T17:58:27.964505Z"
    }
   },
   "outputs": [],
   "source": [
    "def geocode(i):\n",
    "    cdf = df[i*chunk_size:(i+1)*chunk_size]\n",
    "    \n",
    "    safe_print(f'geocoding chunk {i+1}/{n_chunks}')\n",
    "    \n",
    "    istart = time.perf_counter()\n",
    "    k = addressbatch_retry(i, io.StringIO(cdf.to_csv(header=False)))\n",
    "    iend = start_time = time.perf_counter()\n",
    "    \n",
    "    if k:\n",
    "        safe_print(f'chunk {i+1} OK. Partial: {iend-istart:.2f} secs. Total: {(iend-start)/60:.2f} mins')\n",
    "        \n",
    "        with cache_lock:\n",
    "            r_df = pd.DataFrame(k)\n",
    "            r_df = r_df.set_index('id')\n",
    "            m = df[k_columns].merge(r_df, on='id')\n",
    "            cache.update(m.drop_duplicates(k_columns).set_index(k_columns).to_dict('index'))\n",
    "            cachefile.write_bytes(pickle.dumps(cache))\n",
    "        return k\n",
    "    else:\n",
    "        safe_print(f'chunk {i+1} FAILED. Partial: {iend-istart:.2f} secs. Total: {(iend-start)/60:.2f} mins')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T18:03:32.417285Z",
     "start_time": "2021-06-20T17:58:28.037713Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geocoding chunk 1/5\n",
      "geocoding chunk 2/5\n",
      "geocoding chunk 3/5\n",
      "geocoding chunk 4/5\n",
      "geocoding chunk 5/5\n",
      "chunk 5 OK. Partial: 53.99 secs. Total: 0.90 mins\n",
      "chunk 2 OK. Partial: 233.43 secs. Total: 3.89 mins\n",
      "chunk 4 OK. Partial: 278.91 secs. Total: 4.65 mins\n",
      "chunk 1 OK. Partial: 291.81 secs. Total: 4.86 mins\n",
      "chunk 3 OK. Partial: 303.11 secs. Total: 5.05 mins\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "start = time.perf_counter()\n",
    "with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "    results = executor.map(geocode, range(n_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T18:18:00.388403Z",
     "start_time": "2021-06-20T18:18:00.382406Z"
    }
   },
   "outputs": [],
   "source": [
    "results = list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T18:18:00.799155Z",
     "start_time": "2021-06-20T18:18:00.767234Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dataframe with results\n",
    "results_df = pd.concat(map(lambda r: pd.DataFrame(r).set_index('id'), filter(lambda e: e is not None, results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T18:18:01.079609Z",
     "start_time": "2021-06-20T18:18:01.054664Z"
    }
   },
   "outputs": [],
   "source": [
    "# update dataframe with results\n",
    "df.update(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T18:18:01.646816Z",
     "start_time": "2021-06-20T18:18:01.281402Z"
    }
   },
   "outputs": [],
   "source": [
    "fdf = df_with_cache.copy()\n",
    "fdf.update(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T18:18:01.663263Z",
     "start_time": "2021-06-20T18:18:01.649485Z"
    }
   },
   "outputs": [],
   "source": [
    "# select rows that were not geocoded\n",
    "errors_df = fdf[fdf['match'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T18:18:01.849507Z",
     "start_time": "2021-06-20T18:18:01.815066Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geocoding 0 failed addresses\n"
     ]
    }
   ],
   "source": [
    "print(f'geocoding {errors_df.shape[0]} failed addresses')\n",
    "\n",
    "errors_n_chuncks = math.ceil(errors_df.shape[0] / 50)\n",
    "results = []\n",
    "for i in range(errors_n_chuncks):\n",
    "    data = errors_df.iloc[i*50:(i+1)*50][['num_street','city','state','zip_code']].to_csv(header=False)\n",
    "    k = addressbatch_retry(i, io.StringIO(data))\n",
    "    \n",
    "    if k:\n",
    "        print(f'{i+1}/{errors_n_chuncks} OK')\n",
    "        results.append(k)\n",
    "        \n",
    "        print('len cache:', len(cache))\n",
    "        r_df = pd.DataFrame(k)\n",
    "        r_df = r_df.set_index('id')\n",
    "        m = df[k_columns].merge(r_df, on='id')\n",
    "        cache.update(m.drop_duplicates(k_columns).set_index(k_columns).to_dict('index'))\n",
    "        cachefile.write_bytes(pickle.dumps(cache))\n",
    "        print('len cache:', len(cache))\n",
    "    else:\n",
    "        print(f'{i+1}/{errors_n_chuncks} FAILED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T18:18:03.155654Z",
     "start_time": "2021-06-20T18:18:03.148959Z"
    }
   },
   "outputs": [],
   "source": [
    "# update dataframe\n",
    "if results:\n",
    "    errors_results_df = pd.concat(map(lambda r: pd.DataFrame(r).set_index('id'), results))\n",
    "    fdf.update(errors_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-20T18:18:24.277116Z",
     "start_time": "2021-06-20T18:18:22.299305Z"
    }
   },
   "outputs": [],
   "source": [
    "fdf.to_csv('final.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
